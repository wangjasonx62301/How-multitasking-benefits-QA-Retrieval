{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install evaluate\n",
    "# import sys\n",
    "# sys.path.append('/kaggle/input/qwertttt')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "import torch \n",
    "\n",
    "import transformers \n",
    "from transformers import BertTokenizer \n",
    "from typing import * \n",
    "\n",
    "from MEOW_Models.MT_models import MEOW_MTM \n",
    "from MEOW_Models.Kernel_model import BertWithoutEmbedding \n",
    "\n",
    "from MEOW_Utils.Data_utils import* \n",
    "from MEOW_Utils.Training_utils import* \n",
    "from MEOW_Utils.config import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\codeblocks_workspace\\MEOW\\MEOW_Utils\\Data_utils.py:168: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['TKstart'][i], df_train['TKend'][i] = count_the_TKbeg_and_TKend(df_train.iloc[i]['context'], df_train.iloc[i]['answer_start'], df_train.iloc[i]['text'], tokenizer)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>text</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>label</th>\n",
       "      <th>TKstart</th>\n",
       "      <th>TKend</th>\n",
       "      <th>SEP_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56be85543aeaaa14008c9063</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>in the late 1990s</td>\n",
       "      <td>269</td>\n",
       "      <td>1</td>\n",
       "      <td>67</td>\n",
       "      <td>70</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56be85543aeaaa14008c9065</td>\n",
       "      <td>What areas did Beyonce compete in when she was...</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>singing and dancing</td>\n",
       "      <td>207</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>57</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56be85543aeaaa14008c9066</td>\n",
       "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>2003</td>\n",
       "      <td>526</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9601</td>\n",
       "      <td>In what city and state did Beyonce  grow up?</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>houston, texas</td>\n",
       "      <td>166</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>49</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9602</td>\n",
       "      <td>In which decade did Beyonce become famous?</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>late 1990s</td>\n",
       "      <td>276</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>70</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99840</th>\n",
       "      <td>5acd74fd07355d001abf4325</td>\n",
       "      <td>Why wasn't secondary school introduced in the...</td>\n",
       "      <td>Secondary education in the United States did n...</td>\n",
       "      <td></td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99841</th>\n",
       "      <td>5acd74fd07355d001abf4326</td>\n",
       "      <td>Who didn't benefit from secondary schools?</td>\n",
       "      <td>Secondary education in the United States did n...</td>\n",
       "      <td></td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99842</th>\n",
       "      <td>5acd74fd07355d001abf4327</td>\n",
       "      <td>Why were high schools not created?</td>\n",
       "      <td>Secondary education in the United States did n...</td>\n",
       "      <td></td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99843</th>\n",
       "      <td>572b8770111d821400f38ee8</td>\n",
       "      <td>What was another name used for Higher Education?</td>\n",
       "      <td>Higher education, also called tertiary, third ...</td>\n",
       "      <td>tertiary, third stage, or postsecondary education</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99844</th>\n",
       "      <td>572b8770111d821400f38ee9</td>\n",
       "      <td>What is High Education?</td>\n",
       "      <td>Higher education, also called tertiary, third ...</td>\n",
       "      <td>follows the completion of a school such as a h...</td>\n",
       "      <td>126</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>36</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99845 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          index  \\\n",
       "0      56be85543aeaaa14008c9063   \n",
       "1      56be85543aeaaa14008c9065   \n",
       "2      56be85543aeaaa14008c9066   \n",
       "3      56bf6b0f3aeaaa14008c9601   \n",
       "4      56bf6b0f3aeaaa14008c9602   \n",
       "...                         ...   \n",
       "99840  5acd74fd07355d001abf4325   \n",
       "99841  5acd74fd07355d001abf4326   \n",
       "99842  5acd74fd07355d001abf4327   \n",
       "99843  572b8770111d821400f38ee8   \n",
       "99844  572b8770111d821400f38ee9   \n",
       "\n",
       "                                                question  \\\n",
       "0               When did Beyonce start becoming popular?   \n",
       "1      What areas did Beyonce compete in when she was...   \n",
       "2      When did Beyonce leave Destiny's Child and bec...   \n",
       "3          In what city and state did Beyonce  grow up?    \n",
       "4             In which decade did Beyonce become famous?   \n",
       "...                                                  ...   \n",
       "99840   Why wasn't secondary school introduced in the...   \n",
       "99841         Who didn't benefit from secondary schools?   \n",
       "99842                 Why were high schools not created?   \n",
       "99843   What was another name used for Higher Education?   \n",
       "99844                            What is High Education?   \n",
       "\n",
       "                                                 context  \\\n",
       "0      Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "1      Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "2      Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "3      Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "4      Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "...                                                  ...   \n",
       "99840  Secondary education in the United States did n...   \n",
       "99841  Secondary education in the United States did n...   \n",
       "99842  Secondary education in the United States did n...   \n",
       "99843  Higher education, also called tertiary, third ...   \n",
       "99844  Higher education, also called tertiary, third ...   \n",
       "\n",
       "                                                    text  answer_start  label  \\\n",
       "0                                      in the late 1990s           269      1   \n",
       "1                                    singing and dancing           207      1   \n",
       "2                                                   2003           526      1   \n",
       "3                                         houston, texas           166      1   \n",
       "4                                             late 1990s           276      1   \n",
       "...                                                  ...           ...    ...   \n",
       "99840                                                               -1      0   \n",
       "99841                                                               -1      0   \n",
       "99842                                                               -1      0   \n",
       "99843  tertiary, third stage, or postsecondary education            30      1   \n",
       "99844  follows the completion of a school such as a h...           126      1   \n",
       "\n",
       "       TKstart  TKend  SEP_ind  \n",
       "0           67     70      165  \n",
       "1           55     57      165  \n",
       "2          128    128      165  \n",
       "3           47     49      165  \n",
       "4           69     70      165  \n",
       "...        ...    ...      ...  \n",
       "99840        0      0       94  \n",
       "99841        0      0       94  \n",
       "99842        0      0       94  \n",
       "99843        6     16       95  \n",
       "99844       26     36       95  \n",
       "\n",
       "[99845 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODULE_NAME)\n",
    "## use it if you only want to change the datasize of some dataset(dataframe)\n",
    "# create_CoLA_df(ORG_FILE_PATH_CoLA, tokenizer, data_size=CoLA_DATASIZE)\n",
    "# create_MNLI_df(ORG_FILE_PATH_MNLI, tokenizer, data_size=MNLI_DATASIZE)\n",
    "# create_SQuAD_df(ORG_FILE_PATH_SQuAD, tokenizer, data_size=SQuAD_DATASIZE)\n",
    "# create_QNLI_df(ORG_FILE_PATH_QNLI, tokenizer, data_size=QNLI_DATASIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#處理好 dataframe\n",
    "df_CoLA = pd.read_csv(INPUT_FILE_PATH_CoLA, index_col=[0])\n",
    "df_MNLI = pd.read_csv(INPUT_FILE_PATH_MNLI, index_col=[0])\n",
    "df_SQuAD = pd.read_csv(INPUT_FILE_PATH_SQuAD, index_col=[0])\n",
    "df_QNLI = pd.read_csv(INPUT_FILE_PATH_QNLI, index_col=[0])\n",
    "\n",
    "df_SQuAD_HA = df_SQuAD[df_SQuAD.answer_start != -1]\n",
    "df_SQuAD_NA = df_SQuAD[df_SQuAD.answer_start == -1]\n",
    "\n",
    "# df_SQuAD.sample(n=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertWithoutEmbedding: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertWithoutEmbedding from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertWithoutEmbedding from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "Helper = layer_helper(pretrained_module_name=PRETRAINED_MODULE_NAME, device=DEVICE)\n",
    "\n",
    "SQuAD_NA_databox = DataBox(\n",
    "                 dataset_name = 'SQuAD',\n",
    "                 embedding_layer = Helper.get_embedding_layer(individual=False),\n",
    "                 clfmodeling_layer = Helper.get_modelings_layer_clf(individual_pooler=True),\n",
    "                 df_Data = df_SQuAD_NA,\n",
    "                 test_size = TEST_SIZE,\n",
    "                 tokenizer = tokenizer,\n",
    "                 label_nums = 2,\n",
    "                 batch_size = Target_BATCH_SIZE,\n",
    "                 )\n",
    "\n",
    "SQuAD_HA_databox = DataBox(\n",
    "                 dataset_name = 'SQuAD',\n",
    "                 embedding_layer = SQuAD_NA_databox.embedding_layer,\n",
    "                 clfmodeling_layer = SQuAD_NA_databox.clf_modeling_layer,\n",
    "                 df_Data = df_SQuAD_HA,\n",
    "                 test_size = TEST_SIZE,\n",
    "                 tokenizer = tokenizer,\n",
    "                 label_nums = 2,\n",
    "                 batch_size = Target_BATCH_SIZE,\n",
    "                 )\n",
    "\n",
    "CoLA_databox = DataBox(\n",
    "             dataset_name = 'CoLA',\n",
    "             embedding_layer = Helper.get_embedding_layer(individual=False),\n",
    "             clfmodeling_layer = Helper.get_modelings_layer_clf(individual_pooler=True),\n",
    "             df_Data = df_CoLA,\n",
    "             test_size = TEST_SIZE,\n",
    "             tokenizer = tokenizer,\n",
    "             label_nums = 2,\n",
    "             batch_size = Support_BATCH_SIZE\n",
    "             )\n",
    "\n",
    "MNLI_databox = DataBox(\n",
    "             dataset_name = 'MNLI',\n",
    "             embedding_layer = Helper.get_embedding_layer(individual=False),\n",
    "             clfmodeling_layer = Helper.get_modelings_layer_clf(individual_pooler=True),\n",
    "             df_Data = df_MNLI,\n",
    "             test_size = TEST_SIZE,\n",
    "             tokenizer = tokenizer,\n",
    "             label_nums = 3,\n",
    "             batch_size = Support_BATCH_SIZE\n",
    "             )\n",
    "\n",
    "QNLI_databox = DataBox(\n",
    "             dataset_name = 'QNLI',\n",
    "             embedding_layer = Helper.get_embedding_layer(individual=False),\n",
    "             clfmodeling_layer = Helper.get_modelings_layer_clf(individual_pooler=True),\n",
    "             df_Data = df_QNLI,\n",
    "             test_size = TEST_SIZE,\n",
    "             tokenizer = tokenizer,\n",
    "             label_nums = 2,\n",
    "             batch_size = Support_BATCH_SIZE\n",
    "             )\n",
    "\n",
    "support_dataBox_list = [CoLA_databox, MNLI_databox, QNLI_databox]\n",
    "\n",
    "\n",
    "# for i in SQuAD_NA_databox.test_dataloader:\n",
    "#     0\n",
    "# for i in SQuAD_HA_databox.test_dataloader:\n",
    "#     0\n",
    "# for i in MNLI_databox.test_dataloader:\n",
    "#     0\n",
    "# for i in CoLA_databox.test_dataloader:\n",
    "#     0\n",
    "for i in QNLI_databox.test_dataloader:\n",
    "    0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per epoc round's num is 978\n",
      "Per epoc round's num is 420\n"
     ]
    }
   ],
   "source": [
    "MEOW_model = MEOW_MTM(kernel_model=Helper.get_kernel_model(),\n",
    "                      modeling_layer_for_qa=Helper.get_modelings_layer_qa(),\n",
    "                      qa_databox = SQuAD_HA_databox,  #pass HA or NA is same\n",
    "                      support_databox_list = support_dataBox_list,\n",
    "                      device=DEVICE\n",
    "                      ) \n",
    "MEOW_model.to(DEVICE)\n",
    "\n",
    "# H = {\n",
    "#     \"train_loss\": [],\n",
    "#     \"train_acc\": [],\n",
    "#     \"val_loss\":[],\n",
    "#     \"val_acc\": []\n",
    "#     }\n",
    "\n",
    "Training_round = min(len(SQuAD_HA_databox.training_dataloader),\n",
    "                     len(SQuAD_NA_databox.training_dataloader),\n",
    "                     len(MNLI_databox.training_dataloader),\n",
    "                     len(CoLA_databox.training_dataloader),\n",
    "                     len(QNLI_databox.training_dataloader))\n",
    "\n",
    "Test_round = min(len(SQuAD_HA_databox.test_dataloader),\n",
    "                 len(SQuAD_NA_databox.test_dataloader),\n",
    "                 len(MNLI_databox.test_dataloader),\n",
    "                 len(CoLA_databox.test_dataloader),\n",
    "                 len(QNLI_databox.test_dataloader))\n",
    "\n",
    "print(f'Per epoc round\\'s num is {Training_round}')\n",
    "print(f'Per epoc round\\'s num is {Test_round}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([SQuAD_HA_databox.df_test, SQuAD_NA_databox.df_test]).reset_index(drop=True)\n",
    "count_F1_score(MEOW_model, df, tokenizer, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 1 iter :\n",
      "the SQuAD has answer loss is :  12.614734649658203\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 3.28 GiB already allocated; 0 bytes free; 3.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Administrator\\codeblocks_workspace\\MEOW\\QA_train.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrator/codeblocks_workspace/MEOW/QA_train.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m SQuAD_NA_training_iter \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(SQuAD_NA_databox\u001b[39m.\u001b[39mtraining_dataloader)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrator/codeblocks_workspace/MEOW/QA_train.ipynb#W6sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(Training_round):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrator/codeblocks_workspace/MEOW/QA_train.ipynb#W6sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m# SQuAD has_answer\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Administrator/codeblocks_workspace/MEOW/QA_train.ipynb#W6sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     loss, prob, correct \u001b[39m=\u001b[39m QA_running(MEOW_model, SQuAD_HA_training_iter, DEVICE, dataset_ind \u001b[39m=\u001b[39;49m DATA_IND[\u001b[39m'\u001b[39;49m\u001b[39mSQuAD\u001b[39;49m\u001b[39m'\u001b[39;49m], do_optimize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrator/codeblocks_workspace/MEOW/QA_train.ipynb#W6sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mthe SQuAD has answer loss is : \u001b[39m\u001b[39m\"\u001b[39m, loss\u001b[39m.\u001b[39mitem())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrator/codeblocks_workspace/MEOW/QA_train.ipynb#W6sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     SQuAD_HA_training_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\codeblocks_workspace\\MEOW\\MEOW_Utils\\Training_utils.py:62\u001b[0m, in \u001b[0;36mQA_running\u001b[1;34m(MEOW_model, iter, device, dataset_ind, do_optimize, return_toks)\u001b[0m\n\u001b[0;32m     59\u001b[0m token \u001b[39m=\u001b[39m token\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     60\u001b[0m label \u001b[39m=\u001b[39m label\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 62\u001b[0m loss, prob \u001b[39m=\u001b[39m MEOW_model\u001b[39m.\u001b[39;49mmt_forward(dataset_ind \u001b[39m=\u001b[39;49m dataset_ind,\n\u001b[0;32m     63\u001b[0m                                     input_ids \u001b[39m=\u001b[39;49m input_ids, \n\u001b[0;32m     64\u001b[0m                                     mask \u001b[39m=\u001b[39;49m mask, \n\u001b[0;32m     65\u001b[0m                                     token_type_ids \u001b[39m=\u001b[39;49m token,\n\u001b[0;32m     66\u001b[0m                                     SEPind \u001b[39m=\u001b[39;49m SEPind,\n\u001b[0;32m     67\u001b[0m                                     label\u001b[39m=\u001b[39;49mlabel,\n\u001b[0;32m     68\u001b[0m                                     start_pos \u001b[39m=\u001b[39;49m Start_pos,\n\u001b[0;32m     69\u001b[0m                                     end_pos \u001b[39m=\u001b[39;49m End_pos,\n\u001b[0;32m     70\u001b[0m                                     return_toks \u001b[39m=\u001b[39;49m return_toks)\n\u001b[0;32m     72\u001b[0m correct_num \u001b[39m=\u001b[39m count_correct_num(prob, label)\n\u001b[0;32m     73\u001b[0m \u001b[39mif\u001b[39;00m do_optimize:\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\codeblocks_workspace\\MEOW\\MEOW_Models\\MT_models.py:71\u001b[0m, in \u001b[0;36mMEOW_MTM.mt_forward\u001b[1;34m(self, dataset_ind, input_ids, mask, token_type_ids, SEPind, label, start_pos, end_pos, return_toks)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[39mif\u001b[39;00m(dataset_ind \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupport_modulelist\u001b[39m.\u001b[39m\u001b[39m__len__\u001b[39m()): \u001b[39m## is clf task\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupport_modulelist[dataset_ind](input_ids, mask, token_type_ids, SEPind, label)\n\u001b[1;32m---> 71\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mSQuAD_model(input_ids, mask, token_type_ids, SEPind, label, start_pos, end_pos, return_toks)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\codeblocks_workspace\\MEOW\\MEOW_Models\\ST_model.py:131\u001b[0m, in \u001b[0;36mBert_QA.forward\u001b[1;34m(self, input_ids, attention_mask, token, SEPind, label, start_pos, end_pos, return_toks)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[39m####-----------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[39m# this is the embedding output, the QA has answer and no answer use the same embedding layer\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39m# but they use the different modeling layer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    128\u001b[0m \n\u001b[0;32m    129\u001b[0m \u001b[39m#### FIRST AND SECOND LAYER\u001b[39;00m\n\u001b[0;32m    130\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_layer(input_ids\u001b[39m=\u001b[39minput_ids, token_type_ids\u001b[39m=\u001b[39mtoken)\n\u001b[1;32m--> 131\u001b[0m bert_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_model(embedding_output\u001b[39m=\u001b[39;49membedding_output, input_ids\u001b[39m=\u001b[39;49minput_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask, token_type_ids \u001b[39m=\u001b[39;49m token)\n\u001b[0;32m    132\u001b[0m \u001b[39m####-----------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \n\u001b[0;32m    134\u001b[0m \n\u001b[0;32m    135\u001b[0m \u001b[39m#### GET THE QA CLF OUTPUT BY SELF ATTENTION\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m#### ----------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m    137\u001b[0m value_list \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\codeblocks_workspace\\MEOW\\MEOW_Models\\Kernel_model.py:97\u001b[0m, in \u001b[0;36mBertWithoutEmbedding.forward\u001b[1;34m(self, embedding_output, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m     93\u001b[0m     encoder_extended_attention_mask \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     95\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m---> 97\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m     98\u001b[0m     embedding_output,\n\u001b[0;32m     99\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m    100\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    101\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    102\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m    103\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m    104\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    105\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    106\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    107\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    108\u001b[0m )\n\u001b[0;32m    109\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    110\u001b[0m \u001b[39m# pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[39m# we don't use pooler output for kernel model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:610\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    601\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    602\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    603\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    607\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    608\u001b[0m     )\n\u001b[0;32m    609\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 610\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    611\u001b[0m         hidden_states,\n\u001b[0;32m    612\u001b[0m         attention_mask,\n\u001b[0;32m    613\u001b[0m         layer_head_mask,\n\u001b[0;32m    614\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    615\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    616\u001b[0m         past_key_value,\n\u001b[0;32m    617\u001b[0m         output_attentions,\n\u001b[0;32m    618\u001b[0m     )\n\u001b[0;32m    620\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    621\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:537\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    534\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    535\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 537\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[0;32m    538\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[0;32m    539\u001b[0m )\n\u001b[0;32m    540\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[0;32m    542\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\transformers\\pytorch_utils.py:248\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 248\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:549\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    548\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[1;32m--> 549\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintermediate(attention_output)\n\u001b[0;32m    550\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    551\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:450\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m    449\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense(hidden_states)\n\u001b[1;32m--> 450\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintermediate_act_fn(hidden_states)\n\u001b[0;32m    451\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\transformers\\activations.py:78\u001b[0m, in \u001b[0;36mGELUActivation.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m---> 78\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mact(\u001b[39minput\u001b[39;49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 3.28 GiB already allocated; 0 bytes free; 3.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# 訓練\n",
    "for epoch in range(EPOCH_NUM):\n",
    "    print(\"the {:d} iter :\".format(epoch+1))\n",
    "\n",
    "    HA_record = Record()\n",
    "    NA_record = Record()\n",
    "    Sup_record = [Record() for _ in range(SUPPORT_DATA_NUM)]\n",
    "\n",
    "    #### training now \n",
    "    ################################################################################################################\n",
    "    MEOW_model.train()\n",
    "\n",
    "    support_data_training_iter = [iter(db.training_dataloader) for db in support_dataBox_list]\n",
    "    SQuAD_HA_training_iter = iter(SQuAD_HA_databox.training_dataloader)\n",
    "    SQuAD_NA_training_iter = iter(SQuAD_NA_databox.training_dataloader)\n",
    "\n",
    "    for _ in range(Training_round):\n",
    "        # SQuAD has_answer\n",
    "        loss, prob, correct = QA_running(MEOW_model, SQuAD_HA_training_iter, DEVICE, dataset_ind = DATA_IND['SQuAD'], do_optimize=True)\n",
    "        HA_record.add_training_acur(correct, prob.size(0))\n",
    "        HA_record.add_training_loss(loss.item())\n",
    "\n",
    "        # SQuAD no_answer\n",
    "        loss, prob, correct = QA_running(MEOW_model, SQuAD_NA_training_iter, DEVICE, dataset_ind = DATA_IND['SQuAD'], do_optimize=True)\n",
    "        NA_record.add_training_acur(correct, prob.size(0))\n",
    "        NA_record.add_training_loss(loss.item())\n",
    "\n",
    "        #### support data train -------------------------------------\n",
    "        for i in range(SUPPORT_DATA_NUM):\n",
    "            loss, prob, correct = Classifiaction_running(MEOW_model, support_data_training_iter[i], DEVICE, i, do_optimize=True)\n",
    "            Sup_record[i].add_training_acur(correct, prob.size(0))\n",
    "            Sup_record[i].add_training_loss(loss.item())\n",
    "        #### --------------------------------------------------------\n",
    "    \n",
    "    #### record ############################\n",
    "    SQuAD_HA_databox.H['train_loss'].append(HA_record.get_training_average_loss())\n",
    "    SQuAD_HA_databox.H['train_acc'].append(HA_record.get_training_accuracy())\n",
    "    print(\"SQuAD has answer train loss: {:.6f}, correct_rate: {:.4f}\".format(HA_record.get_training_average_loss(), HA_record.get_training_accuracy()))\n",
    "\n",
    "    SQuAD_NA_databox.H['train_loss'].append(NA_record.get_training_average_loss()) \n",
    "    SQuAD_NA_databox.H['train_acc'].append(NA_record.get_training_accuracy())\n",
    "    print(\"SQuAD no answer train loss: {:.6f}, correct_rate: {:.4f}\".format(NA_record.get_training_average_loss(), NA_record.get_training_accuracy()))\n",
    "    \n",
    "    for i in range(SUPPORT_DATA_NUM):\n",
    "        x = Sup_record[i].get_training_average_loss()\n",
    "        y = Sup_record[i].get_training_accuracy()\n",
    "        support_dataBox_list[i].H['train_loss'].append(x)\n",
    "        support_dataBox_list[i].H['train_acc'].append(y)\n",
    "        print(\"{} train loss: {:.6f}, correct_rate: {:.4f}\".format(DATA_NAME[i], x, y))\n",
    "    ########################################\n",
    "\n",
    "    ################################################################################################################\n",
    "\n",
    "\n",
    "    #### test now\n",
    "    ################################################################################################################\n",
    "    MEOW_model.eval()\n",
    "\n",
    "    support_data_test_iter = [iter(db.test_dataloader) for db in support_dataBox_list]\n",
    "    SQuAD_HA_test_iter = iter(SQuAD_HA_databox.test_dataloader)\n",
    "    SQuAD_NA_test_iter = iter(SQuAD_NA_databox.test_dataloader)\n",
    "\n",
    "    for _ in range(Test_round):\n",
    "        # SQuAD has_answer\n",
    "        loss, prob, correct = QA_running(MEOW_model, SQuAD_HA_test_iter, DEVICE, dataset_ind = DATA_IND['SQuAD'], do_optimize=False)\n",
    "        HA_record.add_test_acur(correct, prob.size(0))\n",
    "        HA_record.add_test_loss(loss.item())\n",
    "\n",
    "        # SQuAD no_answer\n",
    "        loss, prob, correct = QA_running(MEOW_model, SQuAD_NA_test_iter, DEVICE, dataset_ind = DATA_IND['SQuAD'], do_optimize=False)\n",
    "        NA_record.add_test_acur(correct, prob.size(0))\n",
    "        NA_record.add_test_loss(loss.item())\n",
    "        \n",
    "        # Sup data \n",
    "        for i in range(SUPPORT_DATA_NUM):\n",
    "            loss, prob, correct = Classifiaction_running(MEOW_model, support_data_test_iter[i], DEVICE, i, do_optimize=False)\n",
    "            Sup_record[i].add_test_acur(correct, prob.size(0))\n",
    "            Sup_record[i].add_test_loss(loss.item())\n",
    "        \n",
    "    #### record ################################\n",
    "    SQuAD_HA_databox.H['test_loss'].append(HA_record.get_test_average_loss())\n",
    "    SQuAD_HA_databox.H['test_acc'].append(HA_record.get_test_accuracy())\n",
    "    print(\"SQuAD has answer test loss: {:.6f}, correct_rate: {:.4f}\".format(HA_record.get_test_average_loss(), HA_record.get_test_accuracy()))\n",
    "\n",
    "    SQuAD_NA_databox.H['test_loss'].append(NA_record.get_test_average_loss()) \n",
    "    SQuAD_NA_databox.H['test_acc'].append(NA_record.get_test_accuracy())\n",
    "    print(\"SQuAD no answer test loss: {:.6f}, correct_rate: {:.4f}\".format(NA_record.get_test_average_loss(), NA_record.get_test_accuracy()))\n",
    "    \n",
    "    for i in range(SUPPORT_DATA_NUM):\n",
    "        x = Sup_record[i].get_test_average_loss()\n",
    "        y = Sup_record[i].get_test_accuracy()\n",
    "        support_dataBox_list[i].H['test_loss'].append(x)\n",
    "        support_dataBox_list[i].H['test_acc'].append(y)\n",
    "        print(\"{} test loss: {:.6f}, correct_rate: {:.4f}\".format(DATA_NAME[i], x, y))\n",
    "    #############################################\n",
    "    print(\" \")\n",
    "    ################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15., 24., 21.],\n",
       "        [10., 12., 11.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[[4,5,6]], [[1,1,1]]], dtype=float)  # 2 * 1 * 3\n",
    "b = torch.tensor([  [[1,2,1],[1,2,1],[1,1,2]], [[0,2,1],[7,7,7],[3,3,3]]  ], dtype=float) # 2 * 3 * 3\n",
    "\n",
    "mtx = torch.matmul(a,b).squeeze()\n",
    "mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[4,5,6], [1,1,1]], dtype=float)  # 2 * 3\n",
    "b = torch.tensor([  [[1,2,1],[1,2,1],[1,1,2]], [[0,2,1],[7,7,7],[3,3,3]]  ], dtype=float) # 2 * 3 * 3\n",
    "\n",
    "mtx = torch.stack( [torch.matmul(a[i], b[i]) for i in range(2) ])\n",
    "mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3])\n",
    "\n",
    "b = a[None, :]\n",
    "\n",
    "print(b.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
