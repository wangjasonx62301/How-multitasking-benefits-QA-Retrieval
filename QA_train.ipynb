{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install evaluate\n",
    "# import sys\n",
    "# sys.path.append('/kaggle/input/qwertttt')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer\n",
    "from typing import *\n",
    "\n",
    "from MEOW_Models.MT_models import MEOW_MTM\n",
    "from MEOW_Models.Kernel_model import BertWithoutEmbedding\n",
    "\n",
    "from MEOW_Utils.Data_utils import*\n",
    "from MEOW_Utils.Training_utils import*\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 8\n",
    "EPOCH_NUM = 10\n",
    "\n",
    "ORG_FILE_PATH_CoLA = r'Dataset_infile\\CoLA_Prompt.csv'\n",
    "ORG_FILE_PATH_MNLI = r'Dataset_infile\\MNLI.csv'\n",
    "ORG_FILE_PATH_SQuAD = r'Dataset_infile\\SQuAD.csv'\n",
    "\n",
    "INPUT_FILE_PATH_CoLA = r'Dataset_infile\\_CoLA.csv'\n",
    "INPUT_FILE_PATH_MNLI = r'Dataset_infile\\_MNLI.csv'\n",
    "INPUT_FILE_PATH_SQuAD = r'Dataset_infile\\_SQuAD.csv'\n",
    "# INPUT_FILE_PATH_SQuAD = r'/kaggle/input/qwertttt/Data_file/SQuAD.csv'\n",
    "\n",
    "SQuAD_DATASIZE = 30000\n",
    "CoLA_DATASIZE = 8550  # max 8550\n",
    "MNLI_DATASIZE = 15000  # max 15000\n",
    "\n",
    "TEST_SIZE = 0.3\n",
    "PRETRAINED_MODULE_NAME = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\codeblocks_workspace\\MEOW\\MEOW_Utils\\Data_utils.py:167: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['TKstart'][i], df_train['TKend'][i] = count_the_TKbeg_and_TKend(df_train.iloc[i]['context'], df_train.iloc[i]['answer_start'], df_train.iloc[i]['text'], tokenizer)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>text</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>label</th>\n",
       "      <th>TKstart</th>\n",
       "      <th>TKend</th>\n",
       "      <th>SEP_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56be85543aeaaa14008c9063</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>in the late 1990s</td>\n",
       "      <td>269</td>\n",
       "      <td>1</td>\n",
       "      <td>67</td>\n",
       "      <td>70</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56be85543aeaaa14008c9065</td>\n",
       "      <td>What areas did Beyonce compete in when she was...</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>singing and dancing</td>\n",
       "      <td>207</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>57</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56be85543aeaaa14008c9066</td>\n",
       "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>2003</td>\n",
       "      <td>526</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9601</td>\n",
       "      <td>In what city and state did Beyonce  grow up?</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>houston, texas</td>\n",
       "      <td>166</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>49</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9602</td>\n",
       "      <td>In which decade did Beyonce become famous?</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>late 1990s</td>\n",
       "      <td>276</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>70</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29971</th>\n",
       "      <td>57065bea75f01819005e7b82</td>\n",
       "      <td>The continuous back and forth regarding the de...</td>\n",
       "      <td>In September 2006, German officials seized MP3...</td>\n",
       "      <td>bringing the patent wild west to germany</td>\n",
       "      <td>365</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>81</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29972</th>\n",
       "      <td>57065e2352bb8914006899f0</td>\n",
       "      <td>Who sued Apple, Samsung Electronics and Sandis...</td>\n",
       "      <td>In February 2007, Texas MP3 Technologies sued ...</td>\n",
       "      <td>texas mp3 technologies</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29973</th>\n",
       "      <td>57065e2352bb8914006899f1</td>\n",
       "      <td>Which court did the lawsuit take place in?</td>\n",
       "      <td>In February 2007, Texas MP3 Technologies sued ...</td>\n",
       "      <td>eastern texas federal court</td>\n",
       "      <td>88</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29974</th>\n",
       "      <td>57065e2352bb8914006899f2</td>\n",
       "      <td>What was the claim that the lawsuit was based on?</td>\n",
       "      <td>In February 2007, Texas MP3 Technologies sued ...</td>\n",
       "      <td>infringement of a portable mp3 player patent</td>\n",
       "      <td>126</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>29</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29975</th>\n",
       "      <td>57065e2352bb8914006899f3</td>\n",
       "      <td>What action did the three companies being sued...</td>\n",
       "      <td>In February 2007, Texas MP3 Technologies sued ...</td>\n",
       "      <td>all settled the claims against them</td>\n",
       "      <td>241</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>51</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29976 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          index  \\\n",
       "0      56be85543aeaaa14008c9063   \n",
       "1      56be85543aeaaa14008c9065   \n",
       "2      56be85543aeaaa14008c9066   \n",
       "3      56bf6b0f3aeaaa14008c9601   \n",
       "4      56bf6b0f3aeaaa14008c9602   \n",
       "...                         ...   \n",
       "29971  57065bea75f01819005e7b82   \n",
       "29972  57065e2352bb8914006899f0   \n",
       "29973  57065e2352bb8914006899f1   \n",
       "29974  57065e2352bb8914006899f2   \n",
       "29975  57065e2352bb8914006899f3   \n",
       "\n",
       "                                                question  \\\n",
       "0               When did Beyonce start becoming popular?   \n",
       "1      What areas did Beyonce compete in when she was...   \n",
       "2      When did Beyonce leave Destiny's Child and bec...   \n",
       "3          In what city and state did Beyonce  grow up?    \n",
       "4             In which decade did Beyonce become famous?   \n",
       "...                                                  ...   \n",
       "29971  The continuous back and forth regarding the de...   \n",
       "29972  Who sued Apple, Samsung Electronics and Sandis...   \n",
       "29973         Which court did the lawsuit take place in?   \n",
       "29974  What was the claim that the lawsuit was based on?   \n",
       "29975  What action did the three companies being sued...   \n",
       "\n",
       "                                                 context  \\\n",
       "0      Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "1      Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "2      Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "3      Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "4      Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "...                                                  ...   \n",
       "29971  In September 2006, German officials seized MP3...   \n",
       "29972  In February 2007, Texas MP3 Technologies sued ...   \n",
       "29973  In February 2007, Texas MP3 Technologies sued ...   \n",
       "29974  In February 2007, Texas MP3 Technologies sued ...   \n",
       "29975  In February 2007, Texas MP3 Technologies sued ...   \n",
       "\n",
       "                                               text  answer_start  label  \\\n",
       "0                                 in the late 1990s           269      1   \n",
       "1                               singing and dancing           207      1   \n",
       "2                                              2003           526      1   \n",
       "3                                    houston, texas           166      1   \n",
       "4                                        late 1990s           276      1   \n",
       "...                                             ...           ...    ...   \n",
       "29971      bringing the patent wild west to germany           365      1   \n",
       "29972                        texas mp3 technologies            18      1   \n",
       "29973                   eastern texas federal court            88      1   \n",
       "29974  infringement of a portable mp3 player patent           126      1   \n",
       "29975           all settled the claims against them           241      1   \n",
       "\n",
       "       TKstart  TKend  SEP_ind  \n",
       "0           67     70      165  \n",
       "1           55     57      165  \n",
       "2          128    128      165  \n",
       "3           47     49      165  \n",
       "4           69     70      165  \n",
       "...        ...    ...      ...  \n",
       "29971       75     81       90  \n",
       "29972        5      7       56  \n",
       "29973       17     20       56  \n",
       "29974       23     29       56  \n",
       "29975       46     51       56  \n",
       "\n",
       "[29976 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODULE_NAME)\n",
    "## use it if you only want to change the datasize of some dataset(dataframe)\n",
    "# create_CoLA_df(ORG_FILE_PATH_CoLA, tokenizer, data_size=CoLA_DATASIZE)\n",
    "# create_MNLI_df(ORG_FILE_PATH_MNLI, tokenizer, data_size=MNLI_DATASIZE)\n",
    "# create_SQuAD_df(ORG_FILE_PATH_SQuAD, tokenizer, SQuAD_DATASIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>text</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>label</th>\n",
       "      <th>TKstart</th>\n",
       "      <th>TKend</th>\n",
       "      <th>SEP_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15942</th>\n",
       "      <td>56e104dfcd28a01900c67447</td>\n",
       "      <td>Where was the Zond 4 over when it was destroye...</td>\n",
       "      <td>The Soviet Zond spacecraft was not yet ready f...</td>\n",
       "      <td>gulf of guinea</td>\n",
       "      <td>558</td>\n",
       "      <td>1</td>\n",
       "      <td>131</td>\n",
       "      <td>133</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25742</th>\n",
       "      <td>56f8ecd49e9bad19000a0709</td>\n",
       "      <td>Hogarth makes no mention of what?</td>\n",
       "      <td>Hogarth then proceeds to say where and why in ...</td>\n",
       "      <td>the classics</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4012</th>\n",
       "      <td>56d10a4717492d1400aab7d8</td>\n",
       "      <td>What technological development led resulted in...</td>\n",
       "      <td>New York City has more than 2,000 arts and cul...</td>\n",
       "      <td>electric lighting</td>\n",
       "      <td>440</td>\n",
       "      <td>1</td>\n",
       "      <td>81</td>\n",
       "      <td>82</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27418</th>\n",
       "      <td>56fae59a8f12f31900630266</td>\n",
       "      <td>In what year was the SFA founded?</td>\n",
       "      <td>Growing out of the Somali people's rich storyt...</td>\n",
       "      <td>1975</td>\n",
       "      <td>264</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20374</th>\n",
       "      <td>56e7b66337bdd419002c43c9</td>\n",
       "      <td>What are the two most prominent theatres in Na...</td>\n",
       "      <td>Most of Nanjing's major theatres are multi-pur...</td>\n",
       "      <td>the people's convention hall and the nanjing a...</td>\n",
       "      <td>165</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>44</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22782</th>\n",
       "      <td>56f8aa0d9b226e1400dd0d97</td>\n",
       "      <td>About how much of UK's passenger traffic did S...</td>\n",
       "      <td>Southampton has always been a port, and the do...</td>\n",
       "      <td>half</td>\n",
       "      <td>254</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24134</th>\n",
       "      <td>56f8542caef2371900625fe4</td>\n",
       "      <td>Did number of cat of arms in the late middle a...</td>\n",
       "      <td>The most notable difference is that, contrary ...</td>\n",
       "      <td>low and did not exceed 200</td>\n",
       "      <td>596</td>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>120</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25839</th>\n",
       "      <td>56f8ff3d9b226e1400dd1244</td>\n",
       "      <td>Who works closely with the definition of the N...</td>\n",
       "      <td>Working closely in conjunction with the defini...</td>\n",
       "      <td>the near east south asia center for strategic ...</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>30</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          index  \\\n",
       "15942  56e104dfcd28a01900c67447   \n",
       "25742  56f8ecd49e9bad19000a0709   \n",
       "4012   56d10a4717492d1400aab7d8   \n",
       "27418  56fae59a8f12f31900630266   \n",
       "20374  56e7b66337bdd419002c43c9   \n",
       "22782  56f8aa0d9b226e1400dd0d97   \n",
       "24134  56f8542caef2371900625fe4   \n",
       "25839  56f8ff3d9b226e1400dd1244   \n",
       "\n",
       "                                                question  \\\n",
       "15942  Where was the Zond 4 over when it was destroye...   \n",
       "25742                  Hogarth makes no mention of what?   \n",
       "4012   What technological development led resulted in...   \n",
       "27418                  In what year was the SFA founded?   \n",
       "20374  What are the two most prominent theatres in Na...   \n",
       "22782  About how much of UK's passenger traffic did S...   \n",
       "24134  Did number of cat of arms in the late middle a...   \n",
       "25839  Who works closely with the definition of the N...   \n",
       "\n",
       "                                                 context  \\\n",
       "15942  The Soviet Zond spacecraft was not yet ready f...   \n",
       "25742  Hogarth then proceeds to say where and why in ...   \n",
       "4012   New York City has more than 2,000 arts and cul...   \n",
       "27418  Growing out of the Somali people's rich storyt...   \n",
       "20374  Most of Nanjing's major theatres are multi-pur...   \n",
       "22782  Southampton has always been a port, and the do...   \n",
       "24134  The most notable difference is that, contrary ...   \n",
       "25839  Working closely in conjunction with the defini...   \n",
       "\n",
       "                                                    text  answer_start  label  \\\n",
       "15942                                     gulf of guinea           558      1   \n",
       "25742                                       the classics            90      1   \n",
       "4012                                   electric lighting           440      1   \n",
       "27418                                               1975           264      1   \n",
       "20374  the people's convention hall and the nanjing a...           165      1   \n",
       "22782                                               half           254      1   \n",
       "24134                         low and did not exceed 200           596      1   \n",
       "25839  the near east south asia center for strategic ...           104      1   \n",
       "\n",
       "       TKstart  TKend  SEP_ind  \n",
       "15942      131    133      173  \n",
       "25742       21     22      118  \n",
       "4012        81     82      158  \n",
       "27418       49     49      246  \n",
       "20374       32     44       63  \n",
       "22782       57     57      232  \n",
       "24134      115    120      136  \n",
       "25839       18     30      151  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#處理好 dataframe\n",
    "df_CoLA = pd.read_csv(INPUT_FILE_PATH_CoLA, index_col=[0])\n",
    "df_MNLI = pd.read_csv(INPUT_FILE_PATH_MNLI, index_col=[0])\n",
    "df_SQuAD = pd.read_csv(INPUT_FILE_PATH_SQuAD, index_col=[0])\n",
    "\n",
    "df_SQuAD_HA = df_SQuAD[df_SQuAD.answer_start != -1]\n",
    "df_SQuAD_NA = df_SQuAD[df_SQuAD.answer_start == -1]\n",
    "\n",
    "df_SQuAD.sample(n=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertWithoutEmbedding: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertWithoutEmbedding from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertWithoutEmbedding from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "Helper = layer_helper(pretrained_module_name=PRETRAINED_MODULE_NAME, device=DEVICE)\n",
    "\n",
    "SQuAD_NA_databox = DataBox(\n",
    "                 dataset_name = 'SQuAD',\n",
    "                 embedding_layer = Helper.get_embedding_layer(individual=True),\n",
    "                 modeling_layer = Helper.get_modelings_layer_clf(individual_pooler=True),\n",
    "                 df_Data = df_SQuAD_NA,\n",
    "                 test_size = TEST_SIZE,\n",
    "                 tokenizer = tokenizer,\n",
    "                 label_nums = 2,\n",
    "                 batch_size = BATCH_SIZE,\n",
    "                 )\n",
    "\n",
    "SQuAD_HA_databox = DataBox(\n",
    "                 dataset_name = 'SQuAD',\n",
    "                 embedding_layer = SQuAD_NA_databox.embedding_layer,\n",
    "                 modeling_layer = SQuAD_NA_databox.modeling_layer,\n",
    "                 df_Data = df_SQuAD_HA,\n",
    "                 test_size = TEST_SIZE,\n",
    "                 tokenizer = tokenizer,\n",
    "                 label_nums = 2,\n",
    "                 batch_size = BATCH_SIZE,\n",
    "                 )\n",
    "\n",
    "CoLA_databox = DataBox(\n",
    "             dataset_name = 'CoLA',\n",
    "             embedding_layer = Helper.get_embedding_layer(individual=True),\n",
    "             modeling_layer = Helper.get_modelings_layer_clf(individual_pooler=True),\n",
    "             df_Data = df_CoLA,\n",
    "             test_size = TEST_SIZE,\n",
    "             tokenizer = tokenizer,\n",
    "             label_nums = 2,\n",
    "             batch_size = BATCH_SIZE\n",
    "             )\n",
    "\n",
    "MNLI_databox = DataBox(\n",
    "             dataset_name = 'MNLI',\n",
    "             embedding_layer = Helper.get_embedding_layer(individual=True),\n",
    "             modeling_layer = Helper.get_modelings_layer_clf(individual_pooler=True),\n",
    "             df_Data = df_MNLI,\n",
    "             test_size = TEST_SIZE,\n",
    "             tokenizer = tokenizer,\n",
    "             label_nums = 3,\n",
    "             batch_size = BATCH_SIZE\n",
    "             )\n",
    "\n",
    "\n",
    "for i in SQuAD_NA_databox.test_dataloader:\n",
    "    0\n",
    "for i in SQuAD_HA_databox.test_dataloader:\n",
    "    0\n",
    "for i in MNLI_databox.test_dataloader:\n",
    "    0\n",
    "for i in CoLA_databox.test_dataloader:\n",
    "    0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per epoc round's num is 154\n",
      "Per epoc round's num is 66\n"
     ]
    }
   ],
   "source": [
    "MEOW_model = MEOW_MTM(Helper.get_kernel_model(),\n",
    "                      modeling_layer_for_qa = Helper.get_modelings_layer_qa(),\n",
    "                      CoLA_databox=CoLA_databox,\n",
    "                      MNLI_databox=MNLI_databox,\n",
    "                      SQuAD_databox=SQuAD_HA_databox, #pass HA or NA is same\n",
    "                      device = DEVICE)\n",
    "\n",
    "# H = {\n",
    "#     \"train_loss\": [],\n",
    "#     \"train_acc\": [],\n",
    "#     \"val_loss\":[],\n",
    "#     \"val_acc\": []\n",
    "#     }\n",
    "\n",
    "Training_a_epoc_data_num = min(len(SQuAD_HA_databox.training_dataset), \n",
    "                               len(SQuAD_NA_databox.training_dataset), \n",
    "                               len(MNLI_databox.training_dataset),\n",
    "                               len(CoLA_databox.training_dataset))\n",
    "Training_round = int((Training_a_epoc_data_num+BATCH_SIZE-1) / BATCH_SIZE)\n",
    "\n",
    "Test_a_epoc_data_num = min(len(SQuAD_HA_databox.test_dataset), \n",
    "                           len(SQuAD_NA_databox.test_dataset),\n",
    "                           len(MNLI_databox.test_dataset),\n",
    "                           len(CoLA_databox.test_dataset))\n",
    "Test_round = int((Test_a_epoc_data_num+BATCH_SIZE-1) / BATCH_SIZE)\n",
    "\n",
    "print(f'Per epoc round\\'s num is {Training_round}')\n",
    "print(f'Per epoc round\\'s num is {Test_round}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'the', 'late', '1990s']\n",
      "[]\n",
      "\n",
      "['singing', 'and', 'dancing']\n",
      "[]\n",
      "\n",
      "['2003']\n",
      "[]\n",
      "\n",
      "['houston', ',', 'texas']\n",
      "[]\n",
      "\n",
      "['late', '1990s']\n",
      "[]\n",
      "\n",
      "['destiny', \"'\", 's', 'child']\n",
      "[]\n",
      "\n",
      "['dangerously', 'in', 'love']\n",
      "[]\n",
      "\n",
      "['mathew', 'knowles']\n",
      "[]\n",
      "\n",
      "['late', '1990s']\n",
      "[]\n",
      "\n",
      "['lead', 'singer']\n",
      "[]\n",
      "\n",
      "['dangerously', 'in', 'love']\n",
      "[]\n",
      "\n",
      "['2003']\n",
      "[]\n",
      "\n",
      "['five']\n",
      "[]\n",
      "\n",
      "['lead', 'singer']\n",
      "[]\n",
      "\n",
      "['dangerously', 'in', 'love']\n",
      "[]\n",
      "\n",
      "['acting']\n",
      "[]\n",
      "\n",
      "['jay', 'z']\n",
      "[]\n",
      "\n",
      "['six']\n",
      "[]\n",
      "\n",
      "['dream', '##girl', '##s']\n",
      "[]\n",
      "\n",
      "['2010']\n",
      "[]\n",
      "\n",
      "['beyonce']\n",
      "[]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Administrator\\codeblocks_workspace\\MEOW\\QA_train.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Administrator/codeblocks_workspace/MEOW/QA_train.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m count_F1_score(MEOW_model, df_SQuAD, tokenizer, DEVICE)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\codeblocks_workspace\\MEOW\\MEOW_Utils\\Training_utils.py:157\u001b[0m, in \u001b[0;36mcount_F1_score\u001b[1;34m(MEOW_model, df, tokenizer, device)\u001b[0m\n\u001b[0;32m    154\u001b[0m Start_pos \u001b[39m=\u001b[39m [Start_pos]\n\u001b[0;32m    155\u001b[0m End_pos \u001b[39m=\u001b[39m [End_pos]\n\u001b[1;32m--> 157\u001b[0m toks \u001b[39m=\u001b[39m MEOW_model\u001b[39m.\u001b[39;49mSQuAD_forward(input_ids\u001b[39m=\u001b[39;49minput_ids, \n\u001b[0;32m    158\u001b[0m                                     mask\u001b[39m=\u001b[39;49mmask, \n\u001b[0;32m    159\u001b[0m                                     token_type_ids\u001b[39m=\u001b[39;49mtoken,\n\u001b[0;32m    160\u001b[0m                                     SEPind\u001b[39m=\u001b[39;49mSEPind,\n\u001b[0;32m    161\u001b[0m                                     label\u001b[39m=\u001b[39;49mlabel,\n\u001b[0;32m    162\u001b[0m                                     start_pos\u001b[39m=\u001b[39;49mStart_pos, \n\u001b[0;32m    163\u001b[0m                                     end_pos\u001b[39m=\u001b[39;49mEnd_pos,\n\u001b[0;32m    164\u001b[0m                                     return_toks\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    167\u001b[0m ans_toks \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mtokenize(df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m][i])\n\u001b[0;32m    168\u001b[0m \u001b[39mprint\u001b[39m(ans_toks)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\codeblocks_workspace\\MEOW\\MEOW_Models\\MT_models.py:119\u001b[0m, in \u001b[0;36mMEOW_MTM.SQuAD_forward\u001b[1;34m(self, input_ids, mask, token_type_ids, SEPind, label, start_pos, end_pos, return_toks)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mSQuAD_forward\u001b[39m(\u001b[39mself\u001b[39m, input_ids, mask, token_type_ids, SEPind, label \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, start_pos \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, end_pos \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, return_toks \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mSQuAD_model(input_ids, mask, token_type_ids, SEPind, label, start_pos, end_pos, return_toks)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\codeblocks_workspace\\MEOW\\MEOW_Models\\ST_model.py:115\u001b[0m, in \u001b[0;36mBert_QA.forward\u001b[1;34m(self, input_ids, attention_mask, token, SEPind, label, start_pos, end_pos, return_toks)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m     93\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     94\u001b[0m     input_ids : torch\u001b[39m.\u001b[39mtensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[39m#### THE ENTIRE MODEL RUN AND OUTPUT --------------------------------------------\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[39m####-----------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_layer(input_ids\u001b[39m=\u001b[39minput_ids, token_type_ids\u001b[39m=\u001b[39mtoken)\n\u001b[1;32m--> 115\u001b[0m     bert_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_model(embedding_output\u001b[39m=\u001b[39;49membedding_output, input_ids\u001b[39m=\u001b[39;49minput_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask, token_type_ids \u001b[39m=\u001b[39;49m token)\n\u001b[0;32m    117\u001b[0m     output_clf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodeling_layer_clf(SEPind, bert_output)  \u001b[39m# (batch_size, 768)\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     output_qa \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodeling_layer_qa(SEPind, bert_output) \u001b[39m# (batch_size, context_length, 768)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\codeblocks_workspace\\MEOW\\MEOW_Models\\Kernel_model.py:97\u001b[0m, in \u001b[0;36mBertWithoutEmbedding.forward\u001b[1;34m(self, embedding_output, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m     93\u001b[0m     encoder_extended_attention_mask \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     95\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m---> 97\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m     98\u001b[0m     embedding_output,\n\u001b[0;32m     99\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m    100\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    101\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    102\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m    103\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m    104\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    105\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    106\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    107\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    108\u001b[0m )\n\u001b[0;32m    109\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    110\u001b[0m \u001b[39m# pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[39m# we don't use pooler output for kernel model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:611\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    602\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    603\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    604\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    608\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    609\u001b[0m     )\n\u001b[0;32m    610\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 611\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    612\u001b[0m         hidden_states,\n\u001b[0;32m    613\u001b[0m         attention_mask,\n\u001b[0;32m    614\u001b[0m         layer_head_mask,\n\u001b[0;32m    615\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    616\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    617\u001b[0m         past_key_value,\n\u001b[0;32m    618\u001b[0m         output_attentions,\n\u001b[0;32m    619\u001b[0m     )\n\u001b[0;32m    621\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    622\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    486\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    487\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    494\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m    495\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    496\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    498\u001b[0m         hidden_states,\n\u001b[0;32m    499\u001b[0m         attention_mask,\n\u001b[0;32m    500\u001b[0m         head_mask,\n\u001b[0;32m    501\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    502\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    503\u001b[0m     )\n\u001b[0;32m    504\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    506\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    418\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    419\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    425\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    426\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 427\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[0;32m    428\u001b[0m         hidden_states,\n\u001b[0;32m    429\u001b[0m         attention_mask,\n\u001b[0;32m    430\u001b[0m         head_mask,\n\u001b[0;32m    431\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    432\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    433\u001b[0m         past_key_value,\n\u001b[0;32m    434\u001b[0m         output_attentions,\n\u001b[0;32m    435\u001b[0m     )\n\u001b[0;32m    436\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[0;32m    437\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:293\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    284\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    285\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    292\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 293\u001b[0m     mixed_query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquery(hidden_states)\n\u001b[0;32m    295\u001b[0m     \u001b[39m# If this is instantiated as a cross-attention module, the keys\u001b[39;00m\n\u001b[0;32m    296\u001b[0m     \u001b[39m# and values come from an encoder; the attention mask needs to be\u001b[39;00m\n\u001b[0;32m    297\u001b[0m     \u001b[39m# such that the encoder's padding tokens are not attended to.\u001b[39;00m\n\u001b[0;32m    298\u001b[0m     is_cross_attention \u001b[39m=\u001b[39m encoder_hidden_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "count_F1_score(MEOW_model, df_SQuAD, tokenizer, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練\n",
    "for epoch in range(EPOCH_NUM):\n",
    "    print(\"the {:d} iter :\".format(epoch+1))\n",
    "\n",
    "    #### training now\n",
    "    ####--------------------------------------------------------------------------------------------------\n",
    "    MEOW_model.train()\n",
    "\n",
    "    SQuAD_HA_training_loss = 0.0\n",
    "    SQuAD_NA_training_loss = 0.0\n",
    "    CoLA_training_loss = 0.0\n",
    "    MNLI_training_loss = 0.0\n",
    "\n",
    "    SQuAD_HA_training_correct = 0\n",
    "    SQuAD_NA_training_correct = 0\n",
    "    CoLA_training_correct = 0\n",
    "    MNLI_training_correct = 0\n",
    "\n",
    "    SQuAD_HA_training_iter = iter(SQuAD_HA_databox.training_dataloader)\n",
    "    SQuAD_NA_training_iter = iter(SQuAD_NA_databox.training_dataloader)\n",
    "    CoLA_training_iter = iter(CoLA_databox.training_dataloader)\n",
    "    MNLI_training_iter = iter(MNLI_databox.training_dataloader)\n",
    "\n",
    "    for i in range(Training_round):\n",
    "        # SQuAD has_answer\n",
    "        # loss, prob, correct = QA_running(MEOW_model, SQuAD_HA_training_iter, DEVICE, 'SQuAD', do_optimize=True)\n",
    "        # print(\"the SQuAD has answer loss is : \", loss.item())\n",
    "        # SQuAD_HA_training_loss += loss.item()\n",
    "        # SQuAD_HA_training_correct += correct\n",
    "\n",
    "        # SQuAD no_answer\n",
    "        # loss, prob, correct = QA_running(MEOW_model, SQuAD_NA_training_iter, DEVICE, 'SQuAD', do_optimize=True)\n",
    "        # print(\"the SQuAD no answer loss is : \", loss.item())\n",
    "        # SQuAD_NA_training_loss += loss.item()\n",
    "        # SQuAD_NA_training_correct += correct\n",
    "\n",
    "        # CoLA\n",
    "        loss, prob, correct = Classifiaction_running(MEOW_model, CoLA_training_iter, DEVICE, 'CoLA', do_optimize=True)\n",
    "        print(\"the CoLA loss is : \", loss.item())\n",
    "        CoLA_training_loss += loss.item()\n",
    "        CoLA_training_correct += correct\n",
    "\n",
    "        # MNLI\n",
    "        loss, prob, correct = Classifiaction_running(MEOW_model, MNLI_training_iter, DEVICE, 'MNLI', do_optimize=True)\n",
    "        print(\"the Sentimnet loss is : \", loss.item())\n",
    "        MNLI_training_loss += loss.item()\n",
    "        MNLI_training_correct += correct\n",
    "        \n",
    "    \n",
    "    SQuAD_HA_avg_loss = SQuAD_HA_training_loss / Training_round\n",
    "    SQuAD_HA_databox.H['train_loss'].append(SQuAD_HA_avg_loss)\n",
    "\n",
    "    SQuAD_NA_avg_loss = SQuAD_NA_training_loss / Training_round\n",
    "    SQuAD_NA_databox.H['train_loss'].append(SQuAD_NA_avg_loss)\n",
    "\n",
    "    CoLA_avg_loss = CoLA_training_loss / Training_round\n",
    "    CoLA_databox.H['train_loss'].append(CoLA_avg_loss)\n",
    "\n",
    "    MNLI_avg_loss = MNLI_training_loss / Training_round\n",
    "    MNLI_databox.H['train_loss'].append(MNLI_avg_loss)\n",
    "\n",
    "    # SQuAD_f1_score = count_F1_score(MEOW_model, SQuAD_NA_databox.df_train, tokenizer, DEVICE)\n",
    "\n",
    "    print(\"SQuAD has answer train loss: {:.6f}, correct_rate: {:.4f}\".format(SQuAD_HA_avg_loss, SQuAD_HA_training_correct/Training_a_epoc_data_num))\n",
    "    print(\"SQuAD no answer train loss: {:.6f}, correct_rate: {:.4f}\".format(SQuAD_NA_avg_loss, SQuAD_NA_training_correct/Training_a_epoc_data_num))\n",
    "    print(\"CoLA train loss: {:.6f}, correct_rate: {:.4f}\".format(CoLA_avg_loss, CoLA_training_correct/Training_a_epoc_data_num))\n",
    "    print(\"MNLI train loss: {:.6f}, correct_rate: {:.4f}\".format(MNLI_avg_loss, MNLI_training_correct/Training_a_epoc_data_num))\n",
    "    ####--------------------------------------------------------------------------------------------------\n",
    "    ####--------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    #### test now\n",
    "    ####--------------------------------------------------------------------------------------------------\n",
    "    MEOW_model.eval()\n",
    "    \n",
    "    SQuAD_HA_test_loss = 0.0\n",
    "    SQuAD_NA_test_loss = 0.0\n",
    "    CoLA_test_loss = 0.0\n",
    "    MNLI_test_loss = 0.0\n",
    "\n",
    "    SQuAD_HA_test_correct = 0\n",
    "    SQuAD_NA_test_correct = 0\n",
    "    CoLA_test_correct = 0\n",
    "    MNLI_test_correct = 0\n",
    "\n",
    "    SQuAD_HA_test_iter = iter(SQuAD_HA_databox.test_dataloader)\n",
    "    SQuAD_NA_test_iter = iter(SQuAD_NA_databox.test_dataloader)\n",
    "    CoLA_test_iter = iter(CoLA_databox.test_dataloader)\n",
    "    MNLI_test_iter = iter(MNLI_databox.test_dataloader)\n",
    "\n",
    "    for i in range(Test_round):\n",
    "        # SQuAD has_answer\n",
    "        loss, prob, correct = QA_running(MEOW_model, SQuAD_HA_test_iter, DEVICE, 'SQuAD', do_optimize=False)\n",
    "        SQuAD_HA_test_loss += loss.item()\n",
    "        SQuAD_HA_test_correct += correct\n",
    "        print(loss.item())\n",
    "\n",
    "        # SQuAD no_answer\n",
    "        loss, prob, correct = QA_running(MEOW_model, SQuAD_NA_test_iter, DEVICE, 'SQuAD', do_optimize=False)\n",
    "        SQuAD_NA_test_loss += loss.item()\n",
    "        SQuAD_NA_test_correct += correct\n",
    "        print(loss.item())\n",
    "\n",
    "        # CoLA\n",
    "        loss, prob, correct = Classifiaction_running(MEOW_model, CoLA_test_iter, DEVICE, 'CoLA', do_optimize=False)\n",
    "        CoLA_test_loss += loss.item()\n",
    "        CoLA_test_correct += correct\n",
    "        print(loss.item())\n",
    "\n",
    "        # MNLI\n",
    "        loss, prob, correct = Classifiaction_running(MEOW_model, MNLI_test_iter, DEVICE, 'MNLI', do_optimize=False)\n",
    "        MNLI_test_loss += loss.item()\n",
    "        MNLI_test_correct += correct\n",
    "        print(loss.item())\n",
    "\n",
    "\n",
    "    SQuAD_HA_avg_loss = SQuAD_HA_test_loss / Test_round\n",
    "    SQuAD_HA_databox.H['test_loss'].append(SQuAD_HA_avg_loss)\n",
    "\n",
    "    SQuAD_NA_avg_loss = SQuAD_NA_test_loss / Test_round\n",
    "    SQuAD_NA_databox.H['test_loss'].append(SQuAD_NA_avg_loss)\n",
    "    \n",
    "    CoLA_avg_loss = CoLA_test_loss / Test_round\n",
    "    CoLA_databox.H['test_loss'].append(CoLA_avg_loss)\n",
    "\n",
    "    MNLI_avg_loss = MNLI_test_loss / Test_round\n",
    "    MNLI_databox.H['test_loss'].append(MNLI_avg_loss)\n",
    "    \n",
    "    # print(\"SQuAD test loss: {:.6f}, SQuAD f1 score: {:.4f}\".format(SQuAD_avg_loss, SQuAD_f1_score))\n",
    "    print(\"SQuAD has answer test loss: {:.6f}, correct_rate: {:.4f}\".format(SQuAD_HA_avg_loss, SQuAD_HA_test_correct/Test_a_epoc_data_num))\n",
    "    print(\"SQuAD no answer test loss: {:.6f}, correct_rate: {:.4f}\".format(SQuAD_NA_avg_loss, SQuAD_NA_test_correct/Test_a_epoc_data_num))\n",
    "    print(\"CoLA test loss: {:.6f}, correct_rate: {:.4f}\".format(CoLA_avg_loss, CoLA_training_correct/Training_a_epoc_data_num))\n",
    "    print(\"MNLI test loss: {:.6f}, correct_rate: {:.4f}\".format(MNLI_avg_loss, MNLI_training_correct/Training_a_epoc_data_num))\n",
    "    print('')\n",
    "    ####--------------------------------------------------------------------------------------------------\n",
    "    ####--------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
